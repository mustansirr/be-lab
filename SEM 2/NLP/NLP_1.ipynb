{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using nltk library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization.\n",
        "Input / Dataset â€“ use any sample sentence"
      ],
      "metadata": {
        "id": "XQxbtwNZqxna"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "744c6f90",
        "outputId": "ce8e4766-f496-4b79-9b29-f81634c7f44e"
      },
      "source": [
        "import nltk # Import the nltk library\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog.\" # Sample Sentence"
      ],
      "metadata": {
        "id": "E2q5AIGHuWNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(sentence) # Whitespace Tokenization\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "KU1ZH4ASv_ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b455cdbf-38c2-4a35-e150-2a60b9caac82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordPunctTokenizer # Punctuation-based Tokenization\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "zusy_D2jvvl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "253e4625-9327-4fd5-a943-fd9a800f3d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import TreebankWordTokenizer # Treebank Tokenization\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IERmmiMguEG2",
        "outputId": "315c5327-01a7-4236-80d5-8076fcec2e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3116bd61"
      },
      "source": [
        "sentence = \"The incredibly quick brown foxes are jumping over the lazy dogs and their cheerfully wagging tails, aren't they? It's 2023!\" # Updated Sample Sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32e4d46d",
        "outputId": "200f0eab-7f5c-44ed-ad40-ed72a350947e"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweeter = TweetTokenizer()\n",
        "tokens_tweet = tweeter.tokenize(sentence)\n",
        "print(tokens_tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'incredibly', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dogs', 'and', 'their', 'cheerfully', 'wagging', 'tails', ',', \"aren't\", 'they', '?', \"It's\", '2023', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea3251cc",
        "outputId": "884cfb53-8be3-46f9-c91c-e2e12b8f6493"
      },
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe_tokenizer = MWETokenizer([('quick', 'brown'), ('lazy', 'dogs'), ('aren', \"'t\"), (\"It's\",)])\n",
        "\n",
        "tokens_mwe = mwe_tokenizer.tokenize(sentence.split())\n",
        "print(tokens_mwe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'incredibly', 'quick_brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy_dogs', 'and', 'their', 'cheerfully', 'wagging', 'tails,', \"aren't\", 'they?', \"It's\", '2023!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fdd4a13",
        "outputId": "85db08b8-0b6b-4312-9684-7de824a86d2b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0df824c",
        "outputId": "77656320-6a95-4fa3-aba6-80a60cabd071"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Assuming tokens_tweet are the base tokens from a previous step\n",
        "stemmed_tokens_porter = [porter_stemmer.stem(token) for token in tokens_tweet]\n",
        "print(stemmed_tokens_porter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'incred', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog', 'and', 'their', 'cheer', 'wag', 'tail', ',', \"aren't\", 'they', '?', \"it'\", '2023', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07fc9a69",
        "outputId": "87e48949-5c99-4153-f4c3-17bc83263050"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "stemmed_tokens_snowball = [snowball_stemmer.stem(token) for token in tokens_tweet]\n",
        "print(stemmed_tokens_snowball)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'incred', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog', 'and', 'their', 'cheer', 'wag', 'tail', ',', \"aren't\", 'they', '?', 'it', '2023', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbef214c",
        "outputId": "43ed23a2-8d55-40e9-c8f4-a256d9fd59a7"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens_tweet]\n",
        "print(lemmatized_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'incredibly', 'quick', 'brown', 'fox', 'are', 'jumping', 'over', 'the', 'lazy', 'dog', 'and', 'their', 'cheerfully', 'wagging', 'tail', ',', \"aren't\", 'they', '?', \"It's\", '2023', '!']\n"
          ]
        }
      ]
    }
  ]
}